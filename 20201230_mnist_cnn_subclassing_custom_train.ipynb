{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# カスタム訓練実装コード\n",
    " - mnist_cnn.py(kerasに付属していたサンプルコード)の改変\n",
    " - 独立版keras → tensorflowのkerasを利用するようにimport等変更\n",
    " - Sequential APIからSubclassing APIでモデル作成するように変更\n",
    " - 最後の方は理解に自信がない部分がある…\n",
    "\n",
    "## メインにソースの参考としたページ\n",
    " - [Tensorflow2(Sequential API, Functional API, Subclassing API)とMNISTではじめる画像分類](https://qiita.com/hiro871_/items/8e8fd65c28d1e2a13fa9#-%E5%AD%A6%E7%BF%92%E6%96%B9%E6%B3%95%E3%83%86%E3%83%BC%E3%83%97%E3%81%AB%E3%82%88%E3%82%8B%E5%AD%A6%E7%BF%92)\n",
    " - [TensorFlow 2.X の使い方を VGG16／ResNet50 の実装と共に解説](https://qiita.com/anieca/items/9dfe3ef46e7b655bf3ee#%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%A0%E8%A8%93%E7%B7%B4-%E5%AE%9F%E8%A3%85)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "class MnistCnnModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MnistCnnModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu')\n",
    "        self.conv2 = Conv2D(64, (3, 3), activation='relu')\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2))\n",
    "        self.drop1 = Dropout(0.25)\n",
    "        self.flat1 = Flatten()\n",
    "        self.dens1 = Dense(128, activation='relu')\n",
    "        self.drop2 = Dropout(0.5)\n",
    "        self.dens2 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.flat1(x)\n",
    "        x = self.dens1(x)\n",
    "        x = self.drop2(x)\n",
    "        return self.dens2(x)\n",
    "\n",
    "model = MnistCnnModel()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "#               optimizer=tf.keras.optimizers.Adadelta(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction='none')\n",
    "optimizer_obj = tf.keras.optimizers.Adadelta()\n",
    "\n",
    "metrics_train = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "metrics_test = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "source": [
    "### model.compileを自力ソースに変更する場合の理解\n",
    "\n",
    "#### lossについて\n",
    " - kerasでは、lossにはfunctionとclassの呼び方2つがある\n",
    "   - function → snake (e.g. `tf.keras.losses.categorical_crossentropy`)\n",
    "   - class → upper camel (e.g. `tf.keras.losses.CategoricalCrossentropy()`)\n",
    " - classだとコンストラクタでreductionが指定できる\n",
    "   - [keras公式ドキュメント](https://keras.io/api/losses/#Standalone-usage-of-losses)の例を見た感じ、<br/>\n",
    "   `reduction='none'`の場合、functionの呼び方でlossを指定した場合と同じ結果を返すと思われる\n",
    "\n",
    "#### metricsについて\n",
    " - compile関数で`metrics=['accuracy']`とした場合、自動で評価関数を何とすべきか判断してくれていた\n",
    "   - `metrics=[tf.keras.metrics.CategoricalAccuracy()]`のような書き方もできる\n",
    " - カスタム訓練の場合で、カスタムじゃない場合と同じ用に値を記録するには<br/>\n",
    " lossとaccuracyで2個、さらにtrain用とtest用で2個で4つオブジェクトが必要っぽい\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  1/12 loss: 2.2635 - accuracy: 0.1934 val_loss: 2.2016 - val_accuracy: 0.3889\n",
      "Epoch  2/12 loss: 2.1545 - accuracy: 0.3414 val_loss: 2.0606 - val_accuracy: 0.5050\n",
      "Epoch  3/12 loss: 2.0018 - accuracy: 0.4498 val_loss: 1.8638 - val_accuracy: 0.6116\n",
      "Epoch  4/12 loss: 1.7972 - accuracy: 0.5382 val_loss: 1.6099 - val_accuracy: 0.6857\n",
      "Epoch  5/12 loss: 1.5645 - accuracy: 0.5977 val_loss: 1.3408 - val_accuracy: 0.7392\n",
      "Epoch  6/12 loss: 1.3458 - accuracy: 0.6403 val_loss: 1.1053 - val_accuracy: 0.7770\n",
      "Epoch  7/12 loss: 1.1705 - accuracy: 0.6745 val_loss: 0.9278 - val_accuracy: 0.8034\n",
      "Epoch  8/12 loss: 1.0440 - accuracy: 0.6985 val_loss: 0.8029 - val_accuracy: 0.8221\n",
      "Epoch  9/12 loss: 0.9520 - accuracy: 0.7214 val_loss: 0.7139 - val_accuracy: 0.8348\n",
      "Epoch 10/12 loss: 0.8783 - accuracy: 0.7370 val_loss: 0.6478 - val_accuracy: 0.8469\n",
      "Epoch 11/12 loss: 0.8269 - accuracy: 0.7500 val_loss: 0.5980 - val_accuracy: 0.8550\n",
      "Epoch 12/12 loss: 0.7824 - accuracy: 0.7620 val_loss: 0.5585 - val_accuracy: 0.8652\n"
     ]
    }
   ],
   "source": [
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           verbose=1,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# 訓練用関数の定義　@tf.functionは処理を早くするために書くやつ\n",
    "@tf.function\n",
    "def train_step(x, t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = loss_obj(t, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer_obj.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    metrics_train(t, predictions)\n",
    "    train_loss(loss)\n",
    "\n",
    "# テスト用関数の定義\n",
    "@tf.function\n",
    "def test_step(x, t):\n",
    "    predictions = model(x)\n",
    "    loss = loss_obj(t, predictions)\n",
    "\n",
    "    metrics_test(t, predictions)\n",
    "    test_loss(loss)\n",
    "\n",
    "# 学習データ準備のコード\n",
    "buffer_size = len(x_train)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# 実際の訓練やテストを実行する(指定したepoch分))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 訓練してテストする\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    # ログ出す\n",
    "    txt = 'Epoch {}/{} loss: {} - accuracy: {} val_loss: {} - val_accuracy: {}'\n",
    "    print(txt.format(\n",
    "        \"{:2d}\".format(epoch + 1),\n",
    "        epochs,\n",
    "        \"{:1.4f}\".format(train_loss.result()),\n",
    "        \"{:1.4f}\".format(metrics_train.result()),\n",
    "        \"{:1.4f}\".format(test_loss.result()),\n",
    "        \"{:1.4f}\".format(metrics_test.result())\n",
    "    ))\n",
    "\n",
    "    # 評価をリセットする リセットしないとすべてのepochでの評価になる？\n",
    "    metrics_train.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    metrics_test.reset_states()\n",
    "    test_loss.reset_states()\n",
    "\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])\n",
    "# model.summary()\n"
   ]
  },
  {
   "source": [
    "### model.fitを自力ソースに変更する場合の理解\n",
    "\n",
    "#### 訓練の流れ　たぶんこんな感じ\n",
    " - `training=True`にして訓練データをモデルにぶっこむ\n",
    " - 損失を計算する\n",
    " - **勾配を計算する**\n",
    " - **勾配に従って値を更新する**\n",
    " - 評価する（accuracy、loss）\n",
    " \n",
    "#### テストの流れ　たぶんこんな感じ\n",
    " - `training=False`(指定しなければデフォルトの値)にして訓練データをモデルにぶっこむ\n",
    " - 損失を計算する\n",
    " - 評価する(accuracy、loss）\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}